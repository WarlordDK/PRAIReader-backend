import os
import torch
from transformers import pipeline, AutoTokenizer, T5ForConditionalGeneration
from typing import Dict, Any, List
from core.config import get_hf_token
import asyncio
import re


class TextAnalyzer:
    def __init__(self):
        self.hf_token = get_hf_token()
        self.model = None
        self.tokenizer = None
        self.models_initialized = False

    async def initialize_models(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        if self.models_initialized:
            return

        print("üîÑ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω–æ–π –º–æ–¥–µ–ª–∏...")

        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–æ–¥–µ–ª—å –∫–æ—Ç–æ—Ä–∞—è —Ç–æ—á–Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å —Ä—É—Å—Å–∫–∏–º
            model_name = "sberbank-ai/rugpt3small_based_on_gpt2"  # –†—É—Å—Å–∫–æ—è–∑—ã—á–Ω–∞—è GPT –º–æ–¥–µ–ª—å

            self.pipeline = pipeline(
                "text-generation",
                model=model_name,
                tokenizer=model_name,
                token=self.hf_token,
                device="cpu",
                torch_dtype=torch.float32,
            )

            self.models_initialized = True
            print("‚úÖ –†—É—Å—Å–∫–æ—è–∑—ã—á–Ω–∞—è –º–æ–¥–µ–ª—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω–æ–π –º–æ–¥–µ–ª–∏: {e}")
            # Fallback –Ω–∞ –æ—á–µ–Ω—å –ø—Ä–æ—Å—Ç—É—é –º–æ–¥–µ–ª—å
            try:
                self.pipeline = pipeline(
                    "text-generation",
                    model="distilgpt2",
                    device="cpu"
                )
                self.models_initialized = True
                print("‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–∞ —Ä–µ–∑–µ—Ä–≤–Ω–∞—è –º–æ–¥–µ–ª—å (–∞–Ω–≥–ª–∏–π—Å–∫–∞—è)")
            except Exception as fallback_error:
                print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–∂–µ —Ä–µ–∑–µ—Ä–≤–Ω—É—é –º–æ–¥–µ–ª—å: {fallback_error}")
                self.models_initialized = False

    def analyze_text(self, text: str) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–∞ —Å–ª–∞–π–¥–∞"""
        if not self.models_initialized:
            return self._get_fallback_analysis(text)

        try:
            # –û—á–∏—â–∞–µ–º —Ç–µ–∫—Å—Ç
            clean_text = self._clean_text(text)

            # –ü–æ–ª—É—á–∞–µ–º –∞–Ω–∞–ª–∏–∑ —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ –ø—Ä–æ–º—Ç–∞–º–∏
            analysis = self._get_meaningful_analysis(clean_text)
            recommendations = self._get_meaningful_recommendations(clean_text)
            problems = self._get_meaningful_problems(clean_text)

            return {
                "main_topic": self._extract_main_topic(clean_text),
                "key_points": self._extract_key_points(clean_text),
                "clarity_score": self._calculate_clarity_score(clean_text),
                "structure_quality": self._assess_structure(clean_text),
                "specific_recommendations": recommendations,
                "problems_detected": problems,
                "llm_analysis": analysis,
                "analysis_type": "russian_llm"
            }

        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–∫—Å—Ç–∞: {e}")
            return self._get_fallback_analysis(text)

    def _clean_text(self, text: str) -> str:
        """–û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞"""
        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø–µ—Ä–µ–Ω–æ—Å—ã –∏ –ø—Ä–æ–±–µ–ª—ã
        text = re.sub(r'\n+', ' ', text)
        text = re.sub(r'\s+', ' ', text)
        return text.strip()[:400]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É

    def _get_meaningful_analysis(self, text: str) -> str:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –æ—Å–º—ã—Å–ª–µ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞"""
        prompt = f"""
        –¢–µ–∫—Å—Ç —Å–ª–∞–π–¥–∞ –ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏–∏: "{text}"

        –ö—Ä–∞—Ç–∫–æ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —ç—Ç–æ—Ç —Ç–µ–∫—Å—Ç. –û —á–µ–º –æ–Ω? –ö–∞–∫–∞—è –æ—Å–Ω–æ–≤–Ω–∞—è –∏–¥–µ—è?
        –ê–Ω–∞–ª–∏–∑:
        """
        return self._safe_llm_call(prompt, 60)

    def _get_meaningful_recommendations(self, text: str) -> List[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π"""
        prompt = f"""
        –¢–µ–∫—Å—Ç —Å–ª–∞–π–¥–∞: "{text}"

        –î–∞–π 2 –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —É–ª—É—á—à–µ–Ω–∏—é —ç—Ç–æ–≥–æ —Å–ª–∞–π–¥–∞. –ë—É–¥—å –∫–æ–Ω–∫—Ä–µ—Ç–µ–Ω.
        –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:
        1.
        """
        response = self._safe_llm_call(prompt, 80)
        return self._parse_meaningful_list(response, "–£–ª—É—á—à–∏—Ç–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∏–∑–ª–æ–∂–µ–Ω–∏—è")

    def _get_meaningful_problems(self, text: str) -> List[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã—Ö –ø—Ä–æ–±–ª–µ–º"""
        prompt = f"""
        –¢–µ–∫—Å—Ç —Å–ª–∞–π–¥–∞: "{text}"

        –ù–∞–π–¥–∏ 2 –æ—Å–Ω–æ–≤–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã –≤ —ç—Ç–æ–º —Ç–µ–∫—Å—Ç–µ –¥–ª—è –ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏–∏.
        –ü—Ä–æ–±–ª–µ–º—ã:
        1.
        """
        response = self._safe_llm_call(prompt, 80)
        return self._parse_meaningful_list(response, "–ü—Ä–æ–≤–µ—Ä—å—Ç–µ —è—Å–Ω–æ—Å—Ç—å –∏–∑–ª–æ–∂–µ–Ω–∏—è")

    def _safe_llm_call(self, prompt: str, max_tokens: int) -> str:
        """–ë–µ–∑–æ–ø–∞—Å–Ω—ã–π –≤—ã–∑–æ–≤ LLM"""
        try:
            # –î–µ–ª–∞–µ–º –ø—Ä–æ–º—Ç –∫–æ—Ä–æ—á–µ –∏ —á–µ—Ç—á–µ
            prompt = prompt.strip()[:600]

            response = self.pipeline(
                prompt,
                max_new_tokens=max_tokens,
                num_return_sequences=1,
                temperature=0.3,  # –ù–∏–∑–∫–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑—É–µ–º–æ—Å—Ç–∏
                do_sample=True,
                pad_token_id=50256,
                truncation=True,
                repetition_penalty=1.3
            )

            if response and len(response) > 0:
                generated_text = response[0]['generated_text']

                # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ –æ—Ç–≤–µ—Ç
                if prompt in generated_text:
                    response_text = generated_text.replace(prompt, "").strip()
                else:
                    response_text = generated_text.strip()

                # –û—á–∏—â–∞–µ–º –æ—Ç–≤–µ—Ç
                return self._clean_response(response_text)

            return ""

        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ LLM: {e}")
            return ""

    def _clean_response(self, text: str) -> str:
        """–û—á–∏—Å—Ç–∫–∞ –æ—Ç–≤–µ—Ç–∞"""
        # –£–±–∏—Ä–∞–µ–º –º—É—Å–æ—Ä–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã
        text = re.sub(r'[^\w\s–∞-—è–ê-–Ø—ë–Å.,!?;:()-]', '', text)
        text = re.sub(r'\s+', ' ', text)
        return text.strip()

    def _parse_meaningful_list(self, response: str, default: str) -> List[str]:
        """–ü–∞—Ä—Å–∏–Ω–≥ —Å–ø–∏—Å–∫–∞"""
        if not response:
            return [default]

        lines = []
        # –†–∞–∑–±–∏–≤–∞–µ–º –ø–æ —Ç–æ—á–∫–∞–º, –ø–µ—Ä–µ–Ω–æ—Å–∞–º, —Ü–∏—Ñ—Ä–∞–º
        for part in re.split(r'[\n\.]', response):
            part = part.strip()
            # –£–±–∏—Ä–∞–µ–º –Ω–æ–º–µ—Ä–∞ –∏ –º–∞—Ä–∫–µ—Ä—ã
            clean_part = re.sub(r'^[\d\-‚Ä¢*]\.?\s*', '', part)
            if clean_part and len(clean_part) > 15 and len(clean_part) < 100:
                lines.append(clean_part)

        return lines[:2] if lines else [default]

    def _extract_main_topic(self, text: str) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–º—ã"""
        sentences = re.split(r'[.!?]+', text)
        if sentences:
            first_sentence = sentences[0].strip()
            if len(first_sentence) > 100:
                return first_sentence[:100] + "..."
            return first_sentence
        return "–¢–µ–º–∞ –Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞"

    def _extract_key_points(self, text: str) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö –ø—É–Ω–∫—Ç–æ–≤"""
        sentences = re.split(r'[.!?]+', text)
        clean_sentences = [s.strip() for s in sentences if s.strip() and len(s.strip()) > 15]
        return clean_sentences[:3] if clean_sentences else ["–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –≤ —Ç–µ–∫—Å—Ç–µ"]

    def _calculate_clarity_score(self, text: str) -> int:
        """–û—Ü–µ–Ω–∫–∞ —è—Å–Ω–æ—Å—Ç–∏"""
        words = text.split()
        sentences = re.split(r'[.!?]+', text)
        clean_sentences = [s for s in sentences if s.strip()]

        if not clean_sentences:
            return 3

        avg_length = len(words) / len(clean_sentences)

        if 10 <= avg_length <= 25:
            return 8
        elif 5 <= avg_length < 10 or 25 < avg_length <= 35:
            return 6
        else:
            return 4

    def _assess_structure(self, text: str) -> str:
        """–û—Ü–µ–Ω–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã"""
        sentences = re.split(r'[.!?]+', text)
        clean_sentences = [s for s in sentences if s.strip()]

        if len(clean_sentences) >= 3:
            return "—Ö–æ—Ä–æ—à–∞—è"
        elif len(clean_sentences) >= 2:
            return "–±–∞–∑–æ–≤–∞—è"
        else:
            return "–º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è"

    def _get_fallback_analysis(self, text: str) -> Dict[str, Any]:
        """–†–µ–∑–µ—Ä–≤–Ω—ã–π –∞–Ω–∞–ª–∏–∑"""
        clean_text = self._clean_text(text)
        return {
            "main_topic": self._extract_main_topic(clean_text),
            "key_points": self._extract_key_points(clean_text),
            "clarity_score": self._calculate_clarity_score(clean_text),
            "structure_quality": self._assess_structure(clean_text),
            "specific_recommendations": ["–î–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ —Ç—Ä–µ–±—É–µ—Ç—Å—è —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω–∞—è –º–æ–¥–µ–ª—å"],
            "problems_detected": ["–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –±–∞–∑–æ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞"],
            "llm_analysis": "–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞",
            "analysis_type": "fallback"
        }


text_analyzer = TextAnalyzer()